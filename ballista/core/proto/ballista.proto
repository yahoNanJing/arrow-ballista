/*
 * Licensed to the Apache Software Foundation (ASF) under one
 * or more contributor license agreements.  See the NOTICE file
 * distributed with this work for additional information
 * regarding copyright ownership.  The ASF licenses this file
 * to you under the Apache License, Version 2.0 (the
 * "License"); you may not use this file except in compliance
 * with the License.  You may obtain a copy of the License at
 * <p>
 * http://www.apache.org/licenses/LICENSE-2.0
 * <p>
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */

syntax = "proto3";

package ballista.protobuf;

option java_multiple_files = true;
option java_package = "org.apache.arrow.ballista.protobuf";
option java_outer_classname = "BallistaProto";

import "datafusion.proto";

///////////////////////////////////////////////////////////////////////////////////////////////////
// Ballista Logical Plan
///////////////////////////////////////////////////////////////////////////////////////////////////

message Statistics {
  int64 num_rows = 1;
  int64 total_byte_size = 2;
  repeated ColumnStats column_stats = 3;
  bool is_exact = 4;
}

message FileRange {
  int64 start = 1;
  int64 end = 2;
}

message PartitionedFile {
  string path = 1;
  uint64 size = 2;
  uint64 last_modified_ns = 3;
  repeated datafusion.ScalarValue partition_values = 4;
  FileRange range = 5;
}

///////////////////////////////////////////////////////////////////////////////////////////////////
// Ballista Physical Plan
///////////////////////////////////////////////////////////////////////////////////////////////////

// PhysicalPlanNode is a nested type
message PhysicalPlanNode {
  oneof PhysicalPlanType {
    ParquetScanExecNode parquet_scan = 1;
    CsvScanExecNode csv_scan = 2;
    EmptyExecNode empty = 3;
    ProjectionExecNode projection = 4;
    GlobalLimitExecNode global_limit = 6;
    LocalLimitExecNode local_limit = 7;
    AggregateExecNode aggregate = 8;
    HashJoinExecNode hash_join = 9;
    ShuffleReaderExecNode shuffle_reader = 10;
    SortExecNode sort = 11;
    CoalesceBatchesExecNode coalesce_batches = 12;
    FilterExecNode filter = 13;
    CoalescePartitionsExecNode merge = 14;
    UnresolvedShuffleExecNode unresolved = 15;
    RepartitionExecNode repartition = 16;
    WindowAggExecNode window = 17;
    ShuffleWriterExecNode shuffle_writer = 18;
    CrossJoinExecNode cross_join = 19;
    AvroScanExecNode avro_scan = 20;
    PhysicalExtensionNode extension = 21;
    UnionExecNode union = 22;
    ExplainExecNode explain = 23;
    SortPreservingMergeExecNode sort_preserving_merge = 24;
  }
}

message PhysicalExtensionNode {
  bytes node = 1;
  repeated PhysicalPlanNode inputs = 2;
}

// physical expressions
message PhysicalExprNode {
  oneof ExprType {
    // column references
    PhysicalColumn column = 1;

    datafusion.ScalarValue literal = 2;

    // binary expressions
    PhysicalBinaryExprNode binary_expr = 3;

    // aggregate expressions
    PhysicalAggregateExprNode aggregate_expr = 4;

    // null checks
    PhysicalIsNull is_null_expr = 5;
    PhysicalIsNotNull is_not_null_expr = 6;
    PhysicalNot not_expr = 7;

    PhysicalCaseNode case_ = 8;
    PhysicalCastNode cast = 9;
    PhysicalSortExprNode sort = 10;
    PhysicalNegativeNode negative = 11;
    PhysicalInListNode in_list = 12;
    PhysicalScalarFunctionNode scalar_function = 13;
    PhysicalTryCastNode try_cast = 14;

    // window expressions
    PhysicalWindowExprNode window_expr = 15;

    PhysicalScalarUdfNode scalar_udf = 16;

    PhysicalDateTimeIntervalExprNode date_time_interval_expr = 17;
  }
}

message PhysicalScalarUdfNode {
  string name = 1;
  repeated PhysicalExprNode args = 2;
  datafusion.ArrowType return_type = 4;
}

message PhysicalAggregateExprNode {
  datafusion.AggregateFunction aggr_function = 1;
  repeated PhysicalExprNode expr = 2;
  bool distinct = 3;
}

message PhysicalWindowExprNode {
  oneof window_function {
    datafusion.AggregateFunction aggr_function = 1;
    datafusion.BuiltInWindowFunction built_in_function = 2;
    // udaf = 3
  }
  PhysicalExprNode expr = 4;
}

message PhysicalIsNull {
  PhysicalExprNode expr = 1;
}

message PhysicalIsNotNull {
  PhysicalExprNode expr = 1;
}

message PhysicalNot {
  PhysicalExprNode expr = 1;
}

message PhysicalAliasNode {
  PhysicalExprNode expr = 1;
  string alias = 2;
}

message PhysicalBinaryExprNode {
  PhysicalExprNode l = 1;
  PhysicalExprNode r = 2;
  string op = 3;
}

message PhysicalDateTimeIntervalExprNode {
  PhysicalExprNode l = 1;
  PhysicalExprNode r = 2;
  string op = 3;
}

message PhysicalSortExprNode {
  PhysicalExprNode expr = 1;
  bool asc = 2;
  bool nulls_first = 3;
}

message PhysicalWhenThen {
  PhysicalExprNode when_expr = 1;
  PhysicalExprNode then_expr = 2;
}

message PhysicalInListNode {
  PhysicalExprNode expr = 1;
  repeated PhysicalExprNode list = 2;
  bool negated = 3;
}

message PhysicalCaseNode {
  PhysicalExprNode expr = 1;
  repeated PhysicalWhenThen when_then_expr = 2;
  PhysicalExprNode else_expr = 3;
}

message PhysicalScalarFunctionNode {
  string name = 1;
  datafusion.ScalarFunction fun = 2;
  repeated PhysicalExprNode args = 3;
  datafusion.ArrowType return_type = 4;
}

message PhysicalTryCastNode {
  PhysicalExprNode expr = 1;
  datafusion.ArrowType arrow_type = 2;
}

message PhysicalCastNode {
  PhysicalExprNode expr = 1;
  datafusion.ArrowType arrow_type = 2;
}

message PhysicalNegativeNode {
  PhysicalExprNode expr = 1;
}

message UnresolvedShuffleExecNode {
  uint32 stage_id = 1;
  datafusion.Schema schema = 2;
  uint32 input_partition_count = 3;
  uint32 output_partition_count = 4;
}

message FilterExecNode {
  PhysicalPlanNode input = 1;
  PhysicalExprNode expr = 2;
}

message FileGroup {
  repeated PartitionedFile files = 1;
}

message ScanLimit {
  // wrap into a message to make it optional
  uint32 limit = 1;
}

message FileScanExecConf {
  repeated FileGroup file_groups = 1;
  datafusion.Schema schema = 2;
  repeated uint32 projection = 4;
  ScanLimit limit = 5;
  Statistics statistics = 6;
  repeated string table_partition_cols = 7;
  string object_store_url = 8;
}

message ParquetScanExecNode {
  FileScanExecConf base_conf = 1;
  datafusion.LogicalExprNode pruning_predicate = 2;
}

message CsvScanExecNode {
  FileScanExecConf base_conf = 1;
  bool has_header = 2;
  string delimiter = 3;
}

message AvroScanExecNode {
  FileScanExecConf base_conf = 1;
}

enum PartitionMode {
  COLLECT_LEFT = 0;
  PARTITIONED = 1;
}

message HashJoinExecNode {
  PhysicalPlanNode left = 1;
  PhysicalPlanNode right = 2;
  repeated JoinOn on = 3;
  datafusion.JoinType join_type = 4;
  PartitionMode partition_mode = 6;
  bool null_equals_null = 7;
  JoinFilter filter = 8;
}

message UnionExecNode {
  repeated PhysicalPlanNode inputs = 1;
}

message ExplainExecNode {
  datafusion.Schema schema = 1;
  repeated datafusion.StringifiedPlan stringified_plans = 2;
  bool verbose = 3;
}

message CrossJoinExecNode {
  PhysicalPlanNode left = 1;
  PhysicalPlanNode right = 2;
}

message PhysicalColumn {
  string name = 1;
  uint32 index = 2;
}

message JoinOn {
  PhysicalColumn left = 1;
  PhysicalColumn right = 2;
}

message EmptyExecNode {
  bool produce_one_row = 1;
  datafusion.Schema schema = 2;
}

message ProjectionExecNode {
  PhysicalPlanNode input = 1;
  repeated PhysicalExprNode expr = 2;
  repeated string expr_name = 3;
}

enum AggregateMode {
  PARTIAL = 0;
  FINAL = 1;
  FINAL_PARTITIONED = 2;
}

message WindowAggExecNode {
  PhysicalPlanNode input = 1;
  repeated PhysicalExprNode window_expr = 2;
  repeated string window_expr_name = 3;
  datafusion.Schema input_schema = 4;
}

message AggregateExecNode {
  repeated PhysicalExprNode group_expr = 1;
  repeated PhysicalExprNode aggr_expr = 2;
  AggregateMode mode = 3;
  PhysicalPlanNode input = 4;
  repeated string group_expr_name = 5;
  repeated string aggr_expr_name = 6;
  // we need the input schema to the partial aggregate to pass to the final aggregate
  datafusion.Schema input_schema = 7;
  repeated PhysicalExprNode null_expr = 8;
  repeated bool groups = 9;
}

message ShuffleWriterExecNode {
  //TODO it seems redundant to provide job and stage id here since we also have them
  // in the TaskDefinition that wraps this plan
  string job_id = 1;
  uint32 stage_id = 2;
  PhysicalPlanNode input = 3;
  PhysicalHashRepartition output_partitioning = 4;
}

message ShuffleReaderExecNode {
  repeated ShuffleReaderPartition partition = 1;
  datafusion.Schema schema = 2;
}

message ShuffleReaderPartition {
  // each partition of a shuffle read can read data from multiple locations
  repeated PartitionLocation location = 1;
}

message GlobalLimitExecNode {
  PhysicalPlanNode input = 1;
  // The number of rows to skip before fetch
  uint32 skip = 2;
  // Maximum number of rows to fetch; negative means no limit
  int64 fetch = 3;
}

message LocalLimitExecNode {
  PhysicalPlanNode input = 1;
  uint32 fetch = 2;
}

message SortExecNode {
  PhysicalPlanNode input = 1;
  repeated PhysicalExprNode expr = 2;
  // Maximum number of highest/lowest rows to fetch; negative means no limit
  int64 fetch = 3;
}

message SortPreservingMergeExecNode {
  PhysicalPlanNode input = 1;
  repeated PhysicalExprNode expr = 2;
}

message CoalesceBatchesExecNode {
  PhysicalPlanNode input = 1;
  uint32 target_batch_size = 2;
}

message CoalescePartitionsExecNode {
  PhysicalPlanNode input = 1;
}

message PhysicalHashRepartition {
  repeated PhysicalExprNode hash_expr = 1;
  uint64 partition_count = 2;
}

message RepartitionExecNode{
  PhysicalPlanNode input = 1;
  oneof partition_method {
    uint64 round_robin = 2;
    PhysicalHashRepartition hash = 3;
    uint64 unknown = 4;
  }
}

message JoinFilter{
  PhysicalExprNode expression = 1;
  repeated ColumnIndex column_indices = 2;
  datafusion.Schema schema = 3;
}

message ColumnIndex{
  uint32 index = 1;
  JoinSide side = 2;
}

enum JoinSide{
  LEFT_SIDE = 0;
  RIGHT_SIDE = 1;
}

///////////////////////////////////////////////////////////////////////////////////////////////////
// Ballista Scheduling
///////////////////////////////////////////////////////////////////////////////////////////////////
message ExecutionGraph {
  string job_id = 1;
  string session_id = 2;
  JobStatus status = 3;
  repeated ExecutionGraphStage stages = 4;
  uint64 output_partitions = 5;
  repeated PartitionLocation output_locations = 6;
  string scheduler_id = 7;
  uint32 task_id_gen = 8;
  repeated StageAttempts failed_attempts = 9;
  string job_name = 10;
  uint64 start_time = 11;
  uint64 end_time = 12;
}

message StageAttempts {
  uint32 stage_id = 1;
  repeated uint32 stage_attempt_num = 2;
}

message ExecutionGraphStage {
  oneof StageType {
      UnResolvedStage unresolved_stage = 1;
      ResolvedStage resolved_stage = 2;
      SuccessfulStage successful_stage = 3;
      FailedStage failed_stage = 4;
  }
}

message UnResolvedStage {
  uint32 stage_id = 1;
  PhysicalHashRepartition output_partitioning = 2;
  repeated uint32 output_links = 3;
  repeated  GraphStageInput inputs = 4;
  bytes plan = 5;
  uint32 stage_attempt_num = 6;
  repeated string last_attempt_failure_reasons = 7;
}

message ResolvedStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  PhysicalHashRepartition output_partitioning = 3;
  repeated uint32 output_links = 4;
  repeated  GraphStageInput inputs = 5;
  bytes plan = 6;
  uint32 stage_attempt_num = 7;
  repeated string last_attempt_failure_reasons = 8;
}

message SuccessfulStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  PhysicalHashRepartition output_partitioning = 3;
  repeated uint32 output_links = 4;
  repeated  GraphStageInput inputs = 5;
  bytes plan = 6;
  repeated TaskInfo task_infos = 7;
  repeated OperatorMetricsSet stage_metrics = 8;
  uint32 stage_attempt_num = 9;
}

message FailedStage {
  uint32 stage_id = 1;
  uint32 partitions = 2;
  PhysicalHashRepartition output_partitioning = 3;
  repeated uint32 output_links = 4;
  bytes plan = 5;
  repeated TaskInfo task_infos = 6;
  repeated OperatorMetricsSet stage_metrics = 7;
  string error_message = 8;
  uint32 stage_attempt_num = 9;
}

message TaskInfo {
  uint32 task_id = 1;
  uint32 partition_id = 2;
  // Scheduler schedule time
  uint64 scheduled_time = 3;
  // Scheduler launch time
  uint64 launch_time = 4;
  // The time the Executor start to run the task
  uint64 start_exec_time = 5;
  // The time the Executor finish the task
  uint64 end_exec_time = 6;
  // Scheduler side finish time
  uint64 finish_time = 7;
  oneof status {
    RunningTask running = 8;
    FailedTask failed = 9;
    SuccessfulTask successful = 10;
  }
}

message GraphStageInput {
  uint32 stage_id = 1;
  repeated TaskInputPartitions partition_locations = 2;
  bool complete = 3;
}

message TaskInputPartitions {
  uint32 partition = 1;
  repeated PartitionLocation partition_location = 2;
}

message KeyValuePair {
  string key = 1;
  string value = 2;
}

message Action {

  oneof ActionType {
    // Fetch a partition from an executor
    FetchPartition fetch_partition = 3;
  }

  // configuration settings
  repeated KeyValuePair settings = 100;
}

message ExecutePartition {
  string job_id = 1;
  uint32 stage_id = 2;
  repeated uint32 partition_id = 3;
  PhysicalPlanNode plan = 4;
  // The task could need to read partitions from other executors
  repeated PartitionLocation partition_location = 5;
  // Output partition for shuffle writer
  PhysicalHashRepartition output_partitioning = 6;
}

message FetchPartition {
  string job_id = 1;
  uint32 stage_id = 2;
  uint32 partition_id = 3;
  string path = 4;
  string host = 5;
  uint32 port = 6;
}

message PartitionLocation {
  // partition_id of the map stage who produces the shuffle.
  uint32 map_partition_id = 1;
  // partition_id of the shuffle, a composition of(job_id + map_stage_id + partition_id).
  PartitionId partition_id = 2;
  ExecutorMetadata executor_meta = 3;
  PartitionStats partition_stats = 4;
  string path = 5;
}

// Unique identifier for a materialized partition of data
message PartitionId {
  string job_id = 1;
  uint32 stage_id = 2;
  uint32 partition_id = 4;
}

message TaskId {
  uint32 task_id = 1;
  uint32 task_attempt_num = 2;
  uint32 partition_id = 3;
}

message PartitionStats {
  int64 num_rows = 1;
  int64 num_batches = 2;
  int64 num_bytes = 3;
  repeated ColumnStats column_stats = 4;
}

message ColumnStats {
  datafusion.ScalarValue min_value = 1;
  datafusion.ScalarValue max_value = 2;
  uint32 null_count = 3;
  uint32 distinct_count = 4;
}

message OperatorMetricsSet {
  repeated OperatorMetric metrics = 1;
}


message NamedCount {
  string name = 1;
  uint64 value = 2;
}

message NamedGauge {
  string name = 1;
  uint64 value = 2;
}

message NamedTime {
  string name = 1;
  uint64 value = 2;
}

message OperatorMetric {
  oneof metric {
    uint64 output_rows = 1;
    uint64 elapse_time = 2;
    uint64 spill_count = 3;
    uint64 spilled_bytes = 4;
    uint64 current_memory_usage = 5;
    NamedCount count = 6;
    NamedGauge gauge = 7;
    NamedTime time = 8;
    int64 start_timestamp = 9;
    int64 end_timestamp = 10;
  }
}

// Used by scheduler
message ExecutorMetadata {
  string id = 1;
  string host = 2;
  uint32 port = 3;
  uint32 grpc_port = 4;
  ExecutorSpecification specification = 5;
}

// Used by grpc
message ExecutorRegistration {
  string id = 1;
  // "optional" keyword is stable in protoc 3.15 but prost is still on 3.14 (see https://github.com/tokio-rs/prost/issues/430 and https://github.com/tokio-rs/prost/pull/455)
  // this syntax is ugly but is binary compatible with the "optional" keyword (see https://stackoverflow.com/questions/42622015/how-to-define-an-optional-field-in-protobuf-3)
  oneof optional_host {
    string host = 2;
  }
  uint32 port = 3;
  uint32 grpc_port = 4;
  ExecutorSpecification specification = 5;
}

message ExecutorHeartbeat {
  string executor_id = 1;
  // Unix epoch-based timestamp in seconds
  uint64 timestamp = 2;
  repeated ExecutorMetric metrics = 3;
  ExecutorStatus status = 4;
}

message ExecutorMetric {
  // TODO add more metrics
  oneof metric {
    uint64 available_memory = 1;
  }
}

message ExecutorStatus {
  oneof status {
    string active = 1;
    string dead = 2;
    string unknown = 3;
  }
}

message ExecutorSpecification {
  repeated ExecutorResource resources = 1;
}

message ExecutorResource {
  // TODO add more resources
  oneof resource {
    uint32 task_slots = 1;
  }
}

message ExecutorData {
  string executor_id = 1;
  repeated ExecutorResourcePair resources = 2;
}

message ExecutorResourcePair {
  ExecutorResource total = 1;
  ExecutorResource available = 2;
}

message RunningTask {
  string executor_id = 1;
}

message FailedTask {
  string error = 1;
  bool retryable = 2;
  // Whether this task failure should be counted to the maximum number of times the task is allowed to retry
  bool count_to_failures = 3;
  oneof failed_reason {
    ExecutionError execution_error = 4;
    FetchPartitionError fetch_partition_error = 5;
    IOError io_error = 6;
    ExecutorLost executor_lost = 7;
    // A successful task's result is lost due to executor lost
    ResultLost result_lost = 8;
    TaskKilled task_killed = 9;
  }
}

message SuccessfulTask {
  string executor_id = 1;
  // TODO tasks are currently always shuffle writes but this will not always be the case
  // so we might want to think about some refactoring of the task definitions
  repeated ShuffleWritePartition partitions = 2;
}

message ExecutionError {
}

message FetchPartitionError {
  string executor_id = 1;
  uint32 map_stage_id = 2;
  uint32 map_partition_id = 3;
}

message IOError {
}

message ExecutorLost {
}

message ResultLost {
}

message TaskKilled {
}

message ShuffleWritePartition {
  uint64 partition_id = 1;
  string path = 2;
  uint64 num_batches = 3;
  uint64 num_rows = 4;
  uint64 num_bytes = 5;
}

message TaskStatus {
  uint32 task_id = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 stage_attempt_num = 4;
  uint32 partition_id = 5;
  uint64 launch_time = 6;
  uint64 start_exec_time = 7;
  uint64 end_exec_time = 8;
  oneof status {
    RunningTask running = 9;
    FailedTask failed = 10;
    SuccessfulTask successful = 11;
  }
  repeated OperatorMetricsSet metrics = 12;
}

message PollWorkParams {
  ExecutorRegistration metadata = 1;
  uint32 num_free_slots = 2;
  // All tasks must be reported until they reach the failed or completed state
  repeated TaskStatus task_status = 3;
}

message TaskDefinition {
  uint32 task_id = 1;
  uint32 task_attempt_num = 2;
  string job_id = 3;
  uint32 stage_id = 4;
  uint32 stage_attempt_num = 5;
  uint32 partition_id = 6;
  bytes plan = 7;
  // Output partition for shuffle writer
  PhysicalHashRepartition output_partitioning = 8;
  string session_id = 9;
  uint64 launch_time = 10;
  repeated KeyValuePair props = 11;
}

// A set of tasks in the same stage
message MultiTaskDefinition {
  repeated TaskId task_ids = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 stage_attempt_num = 4;
  bytes plan = 5;
  // Output partition for shuffle writer
  PhysicalHashRepartition output_partitioning = 6;
  string session_id = 7;
  uint64 launch_time = 8;
  repeated KeyValuePair props = 9;
}

message SessionSettings {
  repeated KeyValuePair configs = 1;
}

message JobSessionConfig {
  string session_id = 1;
  repeated KeyValuePair configs = 2;
}

message PollWorkResult {
  repeated TaskDefinition tasks = 1;
}

message RegisterExecutorParams {
  ExecutorRegistration metadata = 1;
}

message RegisterExecutorResult {
  bool success = 1;
}

message HeartBeatParams {
  string executor_id = 1;
  repeated ExecutorMetric metrics = 2;
  ExecutorStatus status = 3;
}

message HeartBeatResult {
  // TODO it's from Spark for BlockManager
  bool reregister = 1;
}

message StopExecutorParams {
  string executor_id = 1;
  // stop reason
  string reason = 2;
  // force to stop the executor immediately
  bool force = 3;
}

message StopExecutorResult {
}

message ExecutorStoppedParams {
  string executor_id = 1;
  // stop reason
  string reason = 2;
}

message ExecutorStoppedResult {
}

message UpdateTaskStatusParams {
  string executor_id = 1;
  // All tasks must be reported until they reach the failed or completed state
  repeated TaskStatus task_status = 2;
}

message UpdateTaskStatusResult {
  bool success = 1;
}

message ExecuteQueryParams {
  oneof query {
    bytes logical_plan = 1;
    string sql = 2;
  }
  oneof optional_session_id {
    string session_id = 3;
  }
  repeated KeyValuePair settings = 4;
}

message ExecuteSqlParams {
  string sql = 1;
}

message ExecuteQueryResult {
  string job_id = 1;
  string session_id = 2;
}

message GetJobStatusParams {
  string job_id = 1;
}

message SuccessfulJob {
  repeated PartitionLocation partition_location = 1;
}

message QueuedJob {}

// TODO: add progress report
message RunningJob {}

message FailedJob {
  string error = 1;
}

message JobStatus {
  oneof status {
    QueuedJob queued = 1;
    RunningJob running = 2;
    FailedJob failed = 3;
    SuccessfulJob successful = 4;
  }
}

message GetJobStatusResult {
  JobStatus status = 1;
}

message GetFileMetadataParams {
  string path = 1;
  string file_type = 2;
}

message GetFileMetadataResult {
  datafusion.Schema schema = 1;
}

message FilePartitionMetadata {
  repeated string filename = 1;
}

message CancelJobParams {
  string job_id = 1;
}

message CancelJobResult {
  bool cancelled = 1;
}

message CleanJobDataParams {
  string job_id = 1;
}

message CleanJobDataResult {
}

message LaunchTaskParams {
  // Allow to launch a task set to an executor at once
  repeated TaskDefinition tasks = 1;
  string scheduler_id = 2;
}

message LaunchMultiTaskParams {
  // Allow to launch a task set to an executor at once
  repeated MultiTaskDefinition multi_tasks = 1;
  string scheduler_id = 2;
}

message LaunchTaskResult {
  bool success = 1;
  // TODO when part of the task set are scheduled successfully
}

message LaunchMultiTaskResult {
  bool success = 1;
  // TODO when part of the task set are scheduled successfully
}

message CancelTasksParams {
  repeated RunningTaskInfo task_infos = 1;
}

message CancelTasksResult {
  bool cancelled = 1;
}

message RemoveJobDataParams {
  string job_id = 1;
}

message RemoveJobDataResult {
}

message RunningTaskInfo {
  uint32 task_id = 1;
  string job_id = 2;
  uint32 stage_id = 3;
  uint32 partition_id = 4;;
}

service SchedulerGrpc {
  // Executors must poll the scheduler for heartbeat and to receive tasks
  rpc PollWork (PollWorkParams) returns (PollWorkResult) {}

  rpc RegisterExecutor(RegisterExecutorParams) returns (RegisterExecutorResult) {}

  // Push-based task scheduler will only leverage this interface
  // rather than the PollWork interface to report executor states
  rpc HeartBeatFromExecutor (HeartBeatParams) returns (HeartBeatResult) {}

  rpc UpdateTaskStatus (UpdateTaskStatusParams) returns (UpdateTaskStatusResult) {}

  rpc GetFileMetadata (GetFileMetadataParams) returns (GetFileMetadataResult) {}

  rpc ExecuteQuery (ExecuteQueryParams) returns (ExecuteQueryResult) {}

  rpc GetJobStatus (GetJobStatusParams) returns (GetJobStatusResult) {}

  // Used by Executor to tell Scheduler it is stopped.
  rpc ExecutorStopped (ExecutorStoppedParams) returns (ExecutorStoppedResult) {}

  rpc CancelJob (CancelJobParams) returns (CancelJobResult) {}

  rpc CleanJobData (CleanJobDataParams) returns (CleanJobDataResult) {}
}

service ExecutorGrpc {
  rpc LaunchTask (LaunchTaskParams) returns (LaunchTaskResult) {}

  rpc LaunchMultiTask (LaunchMultiTaskParams) returns (LaunchMultiTaskResult) {}

  rpc StopExecutor (StopExecutorParams) returns (StopExecutorResult) {}

  rpc CancelTasks (CancelTasksParams) returns (CancelTasksResult) {}

  rpc RemoveJobData (RemoveJobDataParams) returns (RemoveJobDataResult) {}
}
